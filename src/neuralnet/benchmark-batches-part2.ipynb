{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "# from sklearn.metrics import accuracy_score\n",
    "from Layer import *\n",
    "from network import Network\n",
    "from activation_func import tanh, tanh_prime, sigmoid, sigmoid_prime, softmax, softmax_prime, relu, relu_prime\n",
    "from loss_func import mse, mse_prime, cross_entropy, cross_entropy_prime\n",
    "from data_func import vectorize_labels, k_fold, import_data\n",
    "from performance_func import plot_error, plot_confusion_matrix\n",
    "from network_queue import Queue\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting model 1/8 Batches with early stopping - Fold 1 Layers: [30]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-4bce3fe150af>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;31m#execute queue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mqueue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Results/Extensions\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\seminar\\src\\neuralnet\\network_queue.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, save, folder)\u001b[0m\n\u001b[0;32m     55\u001b[0m                             \u001b[0mval\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"params\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"batch_size\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m                             \u001b[0mval\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"params\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"momentum\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m                             val[\"params\"][\"weight_decay\"])\n\u001b[0m\u001b[0;32m     58\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"accuracies\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"network\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moriginal_test_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Model accuracy \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"accuracies\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\seminar\\src\\neuralnet\\network.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x_train, y_train, x_val, y_val, epochs, learning_rate, batch_size, momentum, weight_decay)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m             \u001b[1;31m# train one epoch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 101\u001b[1;33m             \u001b[0merr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_shuffle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_shuffle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    102\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m             \u001b[1;31m# validate and save to epoch error lists\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\seminar\\src\\neuralnet\\network.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[1;34m(self, x_shuffle, y_shuffle, samples, learning_rate, batch_size, momentum, weight_decay)\u001b[0m\n\u001b[0;32m    153\u001b[0m                 \u001b[1;31m# backprop through all subsequent layers, while also updating parameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m                     \u001b[0moutput_error\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward_propagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_error\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_parameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\seminar\\src\\neuralnet\\Layer.py\u001b[0m in \u001b[0;36mbackward_propagation\u001b[1;34m(self, output_error, learning_rate, batch_size)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;31m# computes dE/dW, dE/dB for a given output_error=dE/dY. Returns input_error=dE/dX.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mbackward_propagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_error\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m         \u001b[0minput_error\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_error\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_gradient\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0moutput_error\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mdot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_error\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# import data and initialize seed\n",
    "np.random.seed(10)\n",
    "\n",
    "# import data\n",
    "training_size = 60000\n",
    "normalize = True\n",
    "training, labels, test, original_test_labels, test_labels = import_data(size=training_size, normalize=normalize)\n",
    "\n",
    "# intializing queue\n",
    "queue = Queue(training, labels, test, test_labels, original_test_labels)\n",
    "\n",
    "#setting up the models\n",
    "\n",
    "\n",
    "architectures = [\n",
    "    [30],[300]]\n",
    "for layers in architectures:\n",
    "    queue.add(\n",
    "        description=\"Batches with early stopping\",\n",
    "        netparams={\n",
    "        \"hidden_layers\": layers,\n",
    "        \"features\": 784,\n",
    "        \"output_classes\": 10,\n",
    "        \"activation\": relu,\n",
    "        \"activation_prime\": relu_prime,\n",
    "        \"loss_activation\": softmax,\n",
    "        \"loss_activation_prime\": softmax_prime,\n",
    "        \"loss\": cross_entropy,\n",
    "        \"loss_prime\": cross_entropy_prime,\n",
    "        \"activation_alpha\": 0.5\n",
    "    },\n",
    "    folds=1,\n",
    "    params={\"epochs\": 200,\n",
    "            \"learning_rate\": 5e-3,\n",
    "            \"batch_size\": 1,\n",
    "            \"momentum\": False,\n",
    "            \"weight_decay\": 0})\n",
    "    \n",
    "    queue.add(\n",
    "        description=\"Batches \",\n",
    "        netparams={\n",
    "        \"hidden_layers\": layers,\n",
    "        \"features\": 784,\n",
    "        \"output_classes\": 10,\n",
    "        \"activation\": relu,\n",
    "        \"activation_prime\": relu_prime,\n",
    "        \"loss_activation\": softmax,\n",
    "        \"loss_activation_prime\": softmax_prime,\n",
    "        \"loss\": cross_entropy,\n",
    "        \"loss_prime\": cross_entropy_prime,\n",
    "        \"activation_alpha\": 0.5\n",
    "    },\n",
    "    folds=1,\n",
    "    params={\"epochs\": 200,\n",
    "            \"learning_rate\": 5e-3,\n",
    "            \"batch_size\": 100,\n",
    "            \"momentum\": False,\n",
    "            \"weight_decay\": 0})\n",
    "    \n",
    "    queue.add(\n",
    "        description=\"Batches \",\n",
    "        netparams={\n",
    "        \"hidden_layers\": layers,\n",
    "        \"features\": 784,\n",
    "        \"output_classes\": 10,\n",
    "        \"activation\": relu,\n",
    "        \"activation_prime\": relu_prime,\n",
    "        \"loss_activation\": softmax,\n",
    "        \"loss_activation_prime\": softmax_prime,\n",
    "        \"loss\": cross_entropy,\n",
    "        \"loss_prime\": cross_entropy_prime,\n",
    "        \"activation_alpha\": 0.5\n",
    "    },\n",
    "    folds=1,\n",
    "    params={\"epochs\": 200,\n",
    "            \"learning_rate\": 5e-3,\n",
    "            \"batch_size\": 100,\n",
    "            \"momentum\": True,\n",
    "            \"weight_decay\": 0})\n",
    "    \n",
    "    queue.add(\n",
    "        description=\"Batches \",\n",
    "        netparams={\n",
    "        \"hidden_layers\": layers,\n",
    "        \"features\": 784,\n",
    "        \"output_classes\": 10,\n",
    "        \"activation\": relu,\n",
    "        \"activation_prime\": relu_prime,\n",
    "        \"loss_activation\": softmax,\n",
    "        \"loss_activation_prime\": softmax_prime,\n",
    "        \"loss\": cross_entropy,\n",
    "        \"loss_prime\": cross_entropy_prime,\n",
    "        \"activation_alpha\": 0.5\n",
    "    },\n",
    "    folds=1,\n",
    "    params={\"epochs\": 200,\n",
    "            \"learning_rate\": 5e-3,\n",
    "            \"batch_size\": 100,\n",
    "            \"momentum\": True,\n",
    "            \"weight_decay\": 0.005})\n",
    "\n",
    "    \n",
    "#execute queue\n",
    "results = queue.execute(folder=\"Results/Extensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
